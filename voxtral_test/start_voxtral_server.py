#!/usr/bin/env python3
"""
Script to start Voxtral-Mini-3B-2507 vLLM server
"""

import subprocess
import sys
from pathlib import Path

import requests


def check_server_status(base_url="http://localhost:8000"):
    """Check if vLLM server is running"""
    try:
        response = requests.get(f"{base_url}/health", timeout=5)
        return response.status_code == 200
    except:
        return False


def check_hf_authentication():
    """Check if Hugging Face authentication is set up"""
    import os

    hf_token = os.getenv("HF_TOKEN")
    if hf_token:
        print(f"âœ… HF_TOKEN found (length: {len(hf_token)})")
        return True
    else:
        print("âŒ HF_TOKEN not found")
        return False


def print_authentication_help():
    """Print help for setting up Hugging Face authentication"""
    print("\nðŸ” Hugging Face Authentication Required")
    print("=" * 50)
    print("The Voxtral model requires authentication to download from Hugging Face.")
    print("\nðŸ“‹ Steps to fix:")
    print("1. Create a Hugging Face account at https://huggingface.co/join")
    print("2. Go to https://huggingface.co/settings/tokens")
    print("3. Create a new token with 'Read' permissions")
    print("4. Set the token as an environment variable:")
    print("   export HF_TOKEN='your_token_here'")
    print("\nðŸ”„ Alternative: Login via CLI")
    print("   huggingface-cli login")
    print("\nðŸ’¡ Then restart this script")


def check_local_model():
    """Check if local model exists"""
    local_model_path = Path("models/voxtral-mini-3b")
    required_files = ["consolidated.safetensors", "params.json", "tekken.json"]

    if not local_model_path.exists():
        return False, "Model directory not found"

    missing_files = []
    for file in required_files:
        if not (local_model_path / file).exists():
            missing_files.append(file)

    if missing_files:
        return False, f"Missing files: {', '.join(missing_files)}"

    return True, str(local_model_path.absolute())


def start_voxtral_server():
    """Start the Voxtral vLLM server"""

    print("ðŸš€ Starting Voxtral-Mini-3B-2507 vLLM Server")
    print("=" * 50)

    # Check if server is already running
    if check_server_status():
        print("âœ… vLLM server is already running!")
        print("ðŸŒ Server URL: http://localhost:8000")
        return

    print("ðŸ’¾ Expected GPU Memory: ~9.5 GB")
    print("ðŸ–¥ï¸  Your System: Mac mini M4 Pro (24 GB unified memory)")
    print("âœ… Memory check: PASSED")

    # Check for local model first
    print("\nðŸ“ Checking for local model...")
    local_available, local_info = check_local_model()

    if local_available:
        print(f"âœ… Local model found: {local_info}")
        model_path = local_info
        use_local = True
    else:
        print(f"âŒ Local model not available: {local_info}")
        print("ðŸ” Checking Hugging Face authentication...")
        if not check_hf_authentication():
            print("\nðŸ’¡ You can download the model manually to avoid authentication:")
            print("   See README.md section 'Option A: Manual Download'")
            print_authentication_help()
            return
        model_path = "mistralai/Voxtral-Mini-3B-2507"
        use_local = False

    print(f"ðŸ“¦ Model: {model_path}")

    # vLLM command
    cmd = [
        "vllm",
        "serve",
        model_path,
        "--tokenizer_mode",
        "mistral",
        "--config_format",
        "mistral",
        "--load_format",
        "mistral",
    ]

    print(f"\nðŸ”§ Command: {' '.join(cmd)}")

    if use_local:
        print(
            "\nâ³ Starting server with local model (this may take 1-2 minutes to load)..."
        )
    else:
        print(
            "\nâ³ Starting server (this may take a few minutes to download and load the model)..."
        )

    try:
        # Start the server
        process = subprocess.Popen(
            cmd,
            stdout=subprocess.PIPE,
            stderr=subprocess.STDOUT,
            universal_newlines=True,
            bufsize=1,
        )

        print("ðŸ”„ Server starting...")

        # Monitor the output
        startup_complete = False
        auth_error_detected = False

        for line in iter(process.stdout.readline, ""):
            print(f"[SERVER] {line.rstrip()}")

            # Check for authentication errors (only relevant for remote models)
            if not use_local and (
                "401 Client Error: Unauthorized" in line
                or "Invalid credentials" in line
            ):
                auth_error_detected = True

            # Check for startup completion indicators
            if "Application startup complete" in line or "Uvicorn running on" in line:
                startup_complete = True
                break

            # Check if process died
            if process.poll() is not None:
                if auth_error_detected:
                    print("\nâŒ Authentication error detected!")
                    print("ðŸ’¡ Consider downloading the model manually (see README.md)")
                    print_authentication_help()
                else:
                    print("âŒ Server process terminated unexpectedly")
                return

        if startup_complete:
            print("\nâœ… Server started successfully!")
            print("ðŸŒ Server URL: http://localhost:8000")
            print("ðŸ“š API docs: http://localhost:8000/docs")
            print("\nðŸŽ¯ Ready to test transcription!")
            print("ðŸ’¡ Run: python test_voxtral_transcription.py")

            # Keep the server running
            print("\nâŒ¨ï¸  Press Ctrl+C to stop the server")
            try:
                process.wait()
            except KeyboardInterrupt:
                print("\nðŸ›‘ Stopping server...")
                process.terminate()
                process.wait()
                print("âœ… Server stopped")

    except FileNotFoundError:
        print("âŒ vLLM not found. Make sure it's installed:")
        print(
            '   uv pip install -U "vllm[audio]" --prerelease=allow --index-strategy unsafe-best-match --extra-index-url https://wheels.vllm.ai/nightly'
        )
    except KeyboardInterrupt:
        print("\nðŸ›‘ Server startup cancelled")
    except Exception as e:
        print(f"âŒ Error starting server: {e}")
        if "401" in str(e) or "Unauthorized" in str(e):
            print_authentication_help()


def show_usage():
    """Show usage instructions"""
    print("ðŸŽ¯ Voxtral-Mini-3B-2507 Test Setup")
    print("=" * 50)
    print("1. ðŸš€ Start server: python start_voxtral_server.py")
    print("2. ðŸ§ª Test transcription: python test_voxtral_transcription.py")
    print("3. ðŸŽ¤ Use JFK sample: ../Voice_Mode_transcript/whisper.cpp/samples/jfk.wav")
    print("\nðŸ“‹ Requirements:")
    print("   - vLLM with audio support installed")
    print("   - mistral_common[audio] installed")
    print("   - ~9.5 GB available GPU memory")


if __name__ == "__main__":
    if len(sys.argv) > 1 and sys.argv[1] == "--help":
        show_usage()
    else:
        start_voxtral_server()
